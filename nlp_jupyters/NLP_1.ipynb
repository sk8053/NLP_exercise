{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets evaluate --upgrade\n",
        "!python -m spacy download de_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sio-5-aT_XhU",
        "outputId": "a0d23a42-8195-42de-87ef-f4888b0d61d6"
      },
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Collecting de-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.7.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "#import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "import spacy\n",
        "import datasets # Hugging face datasets\n",
        "from datasets import Dataset as h_dataset\n",
        "import torchtext\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "seed = 1234\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "Nf8CzzZz_nG-"
      },
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dataset from Huggingface"
      ],
      "metadata": {
        "id": "q56P80sCNAu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " ## load dataset\n",
        "dataset = datasets.load_dataset(\"bentrevett/multi30k\")"
      ],
      "metadata": {
        "id": "L7YEp4B4_sFK"
      },
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print('train data length is {}')\n",
        "print(len(dataset[\"train\"].data['en']))\n",
        "print(dataset[\"train\"]['en'][:10])\n",
        "print(dataset[\"train\"]['de'][:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqXght8nALeL",
        "outputId": "b082676b-0251-433c-e117-3e518d5c9f2d"
      },
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29000\n",
            "['Two young, White males are outside near many bushes.', 'Several men in hard hats are operating a giant pulley system.', 'A little girl climbing into a wooden playhouse.', 'A man in a blue shirt is standing on a ladder cleaning a window.', 'Two men are at the stove preparing food.', 'A man in green holds a guitar while the other man observes his shirt.', 'A man is smiling at a stuffed lion', 'A trendy girl talking on her cellphone while gliding slowly down the street.', 'A woman with a large purse is walking by a gate.', 'Boys dancing on poles in the middle of the night.']\n",
            "['Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.', 'Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.', 'Ein kleines Mädchen klettert in ein Spielhaus aus Holz.', 'Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster.', 'Zwei Männer stehen am Herd und bereiten Essen zu.', 'Ein Mann in grün hält eine Gitarre, während der andere Mann sein Hemd ansieht.', 'Ein Mann lächelt einen ausgestopften Löwen an.', 'Ein schickes Mädchen spricht mit dem Handy während sie langsam die Straße entlangschwebt.', 'Eine Frau mit einer großen Geldbörse geht an einem Tor vorbei.', 'Jungen tanzen mitten in der Nacht auf Pfosten.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    ## load English and German languate with spacy ##\n",
        "    ### spacy is used to process the whole sentence\n",
        "    en_nlp = spacy.load(\"en_core_web_sm\")\n",
        "    de_nlp = spacy.load(\"de_core_news_sm\")"
      ],
      "metadata": {
        "id": "st2T38A0BW0n"
      },
      "execution_count": 296,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://realpython.com/natural-language-processing-spacy-python/\n",
        "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
        "        \"Google in 2007, few people outside of the company took him \"\n",
        "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
        "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
        "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
        "        \"this week.\")\n",
        "doc = en_nlp(text)\n",
        "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
        "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ID3LkPJCCK7D",
        "outputId": "fbfea04b-da18-4208-a2bc-b09f87632122"
      },
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
            "Verbs: ['start', 'work', 'drive', 'take', 'tell', 'shake', 'turn', 'talk', 'say']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing Process\n"
      ],
      "metadata": {
        "id": "bEO7G5OJEm8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en_tokens = [token.text for token in en_nlp.tokenizer(str(dataset[\"train\"][\"en\"][0]))][:max_length]\n",
        "#en_nlp.tokenizer(str(train_data.data[\"en\"][0]))\n",
        "print(en_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXkMNzZyFGow",
        "outputId": "bf6bd26e-a1e8-483c-b58a-e1fe26ad76a4"
      },
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Two', 'young', ',', 'White', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_sentences(data, en_nlp, de_nlp, lower_ch = True, max_length=1000, sos_token='<sos>', eos_token = '<eos>'):\n",
        "  new_data_with_tokens = {'en_tokens':[], 'de_tokens':[]}\n",
        "\n",
        "  for data_en_i, data_de_i in tqdm(zip(data['en'], data['de']), total = len(data['en']), ascii=True, desc = 'number of sentences'):\n",
        "\n",
        "    en_tokens = [token.text for token in en_nlp.tokenizer(str(data_en_i))][:max_length]\n",
        "    de_tokens = [token.text for token in de_nlp.tokenizer(str(data_de_i))][:max_length]\n",
        "    if lower_ch is True:\n",
        "        en_tokens = [token.lower() for token in en_tokens]\n",
        "        de_tokens = [token.lower() for token in de_tokens]\n",
        "\n",
        "    en_tokens = [sos_token] + en_tokens + [eos_token] # append start and end tokens\n",
        "    de_tokens = [sos_token] + de_tokens + [eos_token]\n",
        "\n",
        "    new_data_with_tokens['en_tokens'].append(en_tokens)\n",
        "    new_data_with_tokens['de_tokens'].append(de_tokens)\n",
        "\n",
        "  return new_data_with_tokens"
      ],
      "metadata": {
        "id": "TmDz0PiPJOhZ"
      },
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data, valid_data = dataset[\"train\"], dataset[\"validation\"], dataset['test']\n",
        "#### update train, test, and valid data with tokenizing sentences ###\n",
        "nlp_process_tools = {'en_nlp':en_nlp, 'de_nlp':de_nlp} # NLP processing spacy tools\n",
        "train_data = tokenize_sentences(train_data, **nlp_process_tools)\n",
        "test_data = tokenize_sentences(test_data, **nlp_process_tools)\n",
        "valid_data = tokenize_sentences(valid_data, **nlp_process_tools)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ku_5IKrQHVyl",
        "outputId": "5b245ff0-3c22-400b-fe42-5c22fc72340e"
      },
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "number of sentences: 100%|##########| 29000/29000 [00:06<00:00, 4510.63it/s]\n",
            "number of sentences: 100%|##########| 1014/1014 [00:00<00:00, 3409.88it/s]\n",
            "number of sentences: 100%|##########| 1000/1000 [00:00<00:00, 3633.18it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data[\"en_tokens\"][:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-WG_Z0DQgaK",
        "outputId": "4ecec9ff-ae1c-45a1-cdcb-c8fedbeb5ffc"
      },
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['<sos>', 'two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.', '<eos>'], ['<sos>', 'several', 'men', 'in', 'hard', 'hats', 'are', 'operating', 'a', 'giant', 'pulley', 'system', '.', '<eos>'], ['<sos>', 'a', 'little', 'girl', 'climbing', 'into', 'a', 'wooden', 'playhouse', '.', '<eos>'], ['<sos>', 'a', 'man', 'in', 'a', 'blue', 'shirt', 'is', 'standing', 'on', 'a', 'ladder', 'cleaning', 'a', 'window', '.', '<eos>'], ['<sos>', 'two', 'men', 'are', 'at', 'the', 'stove', 'preparing', 'food', '.', '<eos>'], ['<sos>', 'a', 'man', 'in', 'green', 'holds', 'a', 'guitar', 'while', 'the', 'other', 'man', 'observes', 'his', 'shirt', '.', '<eos>'], ['<sos>', 'a', 'man', 'is', 'smiling', 'at', 'a', 'stuffed', 'lion', '<eos>'], ['<sos>', 'a', 'trendy', 'girl', 'talking', 'on', 'her', 'cellphone', 'while', 'gliding', 'slowly', 'down', 'the', 'street', '.', '<eos>'], ['<sos>', 'a', 'woman', 'with', 'a', 'large', 'purse', 'is', 'walking', 'by', 'a', 'gate', '.', '<eos>'], ['<sos>', 'boys', 'dancing', 'on', 'poles', 'in', 'the', 'middle', 'of', 'the', 'night', '.', '<eos>'], ['<sos>', 'a', 'ballet', 'class', 'of', 'five', 'girls', 'jumping', 'in', 'sequence', '.', '<eos>'], ['<sos>', 'four', 'guys', 'three', 'wearing', 'hats', 'one', 'not', 'are', 'jumping', 'at', 'the', 'top', 'of', 'a', 'staircase', '.', '<eos>'], ['<sos>', 'a', 'black', 'dog', 'and', 'a', 'spotted', 'dog', 'are', 'fighting', '<eos>'], ['<sos>', 'a', 'man', 'in', 'a', 'neon', 'green', 'and', 'orange', 'uniform', 'is', 'driving', 'on', 'a', 'green', 'tractor', '.', '<eos>'], ['<sos>', 'several', 'women', 'wait', 'outside', 'in', 'a', 'city', '.', '<eos>'], ['<sos>', 'a', 'lady', 'in', 'a', 'black', 'top', 'with', 'glasses', 'is', 'sprinkling', 'powdered', 'sugar', 'on', 'a', 'bundt', 'cake', '.', '<eos>'], ['<sos>', 'a', 'little', 'girl', 'is', 'sitting', 'in', 'front', 'of', 'a', 'large', 'painted', 'rainbow', '.', '<eos>'], ['<sos>', 'a', 'man', 'lays', 'on', 'the', 'bench', 'to', 'which', 'a', 'white', 'dog', 'is', 'also', 'tied', '.', '<eos>'], ['<sos>', 'five', 'people', 'are', 'sitting', 'in', 'a', 'circle', 'with', 'instruments', '.', '<eos>'], ['<sos>', 'a', 'bunch', 'of', 'elderly', 'women', 'play', 'their', 'clarinets', 'together', 'as', 'they', 'read', 'off', 'sheet', 'music', '.', '<eos>'], ['<sos>', 'a', 'large', 'structure', 'has', 'broken', 'and', 'is', 'laying', 'in', 'a', 'roadway', '.', '<eos>'], ['<sos>', 'a', 'large', 'crowd', 'of', 'people', 'stand', 'outside', 'in', 'front', 'of', 'the', 'entrance', 'to', 'a', 'metro', 'station', '.', '<eos>'], ['<sos>', 'a', 'man', 'getting', 'a', 'tattoo', 'on', 'his', 'back', '.', '<eos>'], ['<sos>', 'two', 'children', 'sit', 'on', 'a', 'small', 'seesaw', 'in', 'the', 'sand', '.', '<eos>'], ['<sos>', 'a', 'man', 'wearing', 'a', 'reflective', 'vest', 'and', 'a', 'hard', 'hat', 'holds', 'a', 'flag', 'in', 'the', 'road', '<eos>'], ['<sos>', 'a', 'person', 'dressed', 'in', 'a', 'blue', 'coat', 'is', 'standing', 'in', 'on', 'a', 'busy', 'sidewalk', ',', 'studying', 'painting', 'of', 'a', 'street', 'scene', '.', '<eos>'], ['<sos>', 'a', 'man', 'in', 'green', 'pants', 'walking', 'down', 'the', 'road', '.', '<eos>'], ['<sos>', 'the', 'small', 'child', 'climbs', 'on', 'a', 'red', 'ropes', 'on', 'a', 'playground', '.', '<eos>'], ['<sos>', 'you', 'know', 'i', 'am', 'looking', 'like', 'justin', 'bieber', '.', '<eos>'], ['<sos>', 'a', 'young', 'man', 'in', 'a', 'black', 'and', 'yellow', 'jacket', 'is', 'gazing', 'at', 'something', 'and', 'smiling', '.', '<eos>'], ['<sos>', 'a', 'man', 'standing', 'at', 'a', 'urinal', 'with', 'a', 'coffee', 'cup', '.', '<eos>'], ['<sos>', 'five', 'people', 'walking', 'with', 'a', 'multicolored', 'sky', 'in', 'the', 'background', '.', '<eos>'], ['<sos>', 'a', 'old', 'man', 'having', 'a', 'beer', 'alone', '.', '<eos>'], ['<sos>', 'a', 'trained', 'police', 'dog', 'sits', 'next', 'to', 'his', 'handler', 'in', 'front', 'of', 'the', 'police', 'van', '.', '<eos>'], ['<sos>', 'a', 'person', 'riding', 'a', 'bike', 'on', 'a', 'snowy', 'road', '.', '<eos>'], ['<sos>', 'five', 'men', ',', 'uniformly', 'dressed', 'in', 'white', 'shirts', ',', 'tie', 'and', 'black', 'slacks', 'converse', 'at', 'the', 'back', 'of', 'an', 'open', 'van', '.', '<eos>'], ['<sos>', 'a', 'man', 'with', 'a', 'backwards', 'hat', 'works', 'on', 'machinery', '.', '<eos>'], ['<sos>', 'a', 'black', 'woman', 'and', 'a', 'white', 'man', 'working', 'in', 'a', 'factory', 'setting', 'packing', 'jars', 'with', 'candles', 'into', 'boxes', '.', '<eos>'], ['<sos>', 'asian', 'man', 'sweeping', 'the', 'walkway', '.', '<eos>'], ['<sos>', 'a', 'man', 'leans', 'into', 'a', 'car', 'to', 'talk', 'to', 'the', 'driver', ',', 'as', 'a', 'man', 'on', 'a', 'bicycle', 'looks', 'on', '.', '<eos>'], ['<sos>', 'two', 'young', 'toddlers', 'outside', 'on', 'the', 'grass', '.', '<eos>'], ['<sos>', 'people', 'are', 'watching', 'a', 'person', 'in', 'a', 'weird', 'vehicle', 'in', 'a', 'plaza', '.', '<eos>'], ['<sos>', 'a', 'man', 'walks', 'by', 'a', 'silver', 'vehicle', '.', '<eos>'], ['<sos>', 'a', 'beautiful', 'bride', 'walking', 'on', 'a', 'sidewalk', 'with', 'her', 'new', 'husband', '.', '<eos>'], ['<sos>', 'a', 'little', 'boy', 'playing', 'gamecube', 'at', 'a', 'mcdonald', \"'s\", '.', '<eos>'], ['<sos>', 'a', 'white', 'dog', 'shakes', 'on', 'the', 'edge', 'of', 'a', 'beach', 'with', 'an', 'orange', 'ball', '.', '<eos>'], ['<sos>', 'a', 'group', 'of', 'people', 'having', 'a', 'barbecue', 'at', 'a', 'park', '.', '<eos>'], ['<sos>', 'a', 'man', 'in', 'sunglasses', 'puts', 'his', 'arm', 'around', 'a', 'woman', 'in', 'a', 'black', 'and', 'white', 'blouse', '.', '<eos>'], ['<sos>', 'a', 'man', 'with', 'a', 'balloon', 'hat', 'and', 'people', 'eating', 'outdoors', 'at', 'picnic', 'tables', '.', '<eos>'], ['<sos>', 'a', 'boy', 'jump', 'kicking', 'over', 'three', 'kids', 'kicking', 'wood', 'during', 'a', 'tae', 'kwon', 'do', 'competition', '.', '<eos>'], ['<sos>', 'a', 'boy', 'in', 'a', 'red', 'jacket', 'pouring', 'water', 'on', 'a', 'man', 'in', 'a', 'white', 'shirt', '<eos>'], ['<sos>', 'a', 'man', 'with', 'a', 'red', 'jacket', 'is', 'shielding', 'himself', 'from', 'the', 'sun', 'trying', 'to', 'read', 'a', 'piece', 'of', 'paper', '.', '<eos>'], ['<sos>', 'men', 'walking', 'down', 'a', 'street', 'with', 'children', '.', '<eos>'], ['<sos>', 'a', 'little', 'boy', 'is', 'standing', 'on', 'the', 'street', 'while', 'a', 'man', 'in', 'overalls', 'is', 'working', 'on', 'a', 'stone', 'wall', '.', '<eos>'], ['<sos>', 'a', 'black', 'dog', 'leaps', 'over', 'a', 'log', '.', '<eos>'], ['<sos>', 'a', 'man', 'in', 'a', 'suit', 'is', 'running', 'past', 'two', 'other', 'gentleman', ',', 'also', 'dressed', 'in', 'a', 'suit', '.', '<eos>'], ['<sos>', 'man', 'in', 'a', 'red', 'shirt', 'riding', 'his', 'bicycle', 'around', 'water', '.', '<eos>'], ['<sos>', 'a', 'barefooted', 'man', 'wearing', 'olive', 'green', 'shorts', 'grilling', 'hotdogs', 'on', 'a', 'small', 'propane', 'grill', 'while', 'holding', 'a', 'blue', 'plastic', 'cup', '.', '<eos>'], ['<sos>', 'a', 'dog', 'is', 'running', 'in', 'the', 'snow', '<eos>'], ['<sos>', 'a', 'crowd', 'is', 'standing', 'and', 'waiting', 'for', 'the', 'green', 'light', '.', '<eos>'], ['<sos>', 'man', 'on', 'skis', 'looking', 'at', 'artwork', 'for', 'sale', 'in', 'the', 'snow', '<eos>'], ['<sos>', 'seven', 'climbers', 'are', 'ascending', 'a', 'rock', 'face', 'whilst', 'another', 'man', 'stands', 'holding', 'the', 'rope', '.', '<eos>'], ['<sos>', 'the', 'young', 'gymnast', \"'s\", 'supple', 'body', 'soars', 'above', 'the', 'balance', 'beam', '.', '<eos>'], ['<sos>', 'a', 'young', 'boy', 'is', 'pushing', 'a', 'toy', 'atv', 'around', 'a', 'rubber', 'pool', '<eos>'], ['<sos>', 'woman', 'in', 'red', 'windbreaker', 'looking', 'though', 'a', 'rooftop', 'binoculars', 'at', 'the', 'city', 'below', '.', '<eos>'], ['<sos>', 'a', 'man', 'is', 'standing', 'in', 'front', 'of', 'a', 'small', 'red', 'object', 'that', 'looks', 'like', 'a', 'plane', '.', '<eos>'], ['<sos>', 'a', 'dog', 'is', 'playing', 'with', 'a', 'hose', '.', '<eos>'], ['<sos>', 'a', 'man', 'and', 'a', 'little', 'girl', 'happily', 'posing', 'in', 'front', 'of', 'their', 'cart', 'in', 'a', 'supermarket', '.', '<eos>'], ['<sos>', 'a', 'white', 'dog', 'is', 'about', 'to', 'catch', 'a', 'yellow', 'dog', 'toy', '.', '<eos>'], ['<sos>', 'guy', 'in', 'green', 'shirt', 'with', 'hand', 'covering', 'part', 'of', 'his', 'face', 'in', 'restaurant', 'booth', '.', '<eos>'], ['<sos>', 'a', 'black', 'and', 'white', 'dog', 'jumps', 'up', 'towards', 'a', 'yellow', 'toy', '.', '<eos>'], ['<sos>', 'two', 'hikers', 'resting', 'by', 'a', 'patch', 'of', 'snow', '.', '<eos>'], ['<sos>', 'a', 'man', 'showing', 'off', 'his', 'new', 'wooden', 'creation', '.', '<eos>'], ['<sos>', 'a', 'elderly', 'father', 'and', 'his', 'grown', 'son', 'are', 'preparing', 'for', 'a', 'camping', 'trip', 'in', 'the', 'wild', '.', '<eos>'], ['<sos>', 'a', 'bearded', 'traveler', 'in', 'a', 'red', 'shirt', 'sitting', 'in', 'a', 'car', 'and', 'reading', 'a', 'map', '.', '<eos>'], ['<sos>', 'a', 'young', 'boy', 'waves', 'his', 'hand', 'at', 'the', 'duck', 'in', 'the', 'water', 'surrounded', 'by', 'a', 'green', 'park', '.', '<eos>'], ['<sos>', 'a', 'couple', 'sit', 'on', 'the', 'grass', 'with', 'a', 'baby', 'and', 'stroller', '.', '<eos>'], ['<sos>', 'some', 'men', 'standing', 'in', 'front', 'of', 'a', 'building', 'next', 'to', 'a', 'parked', 'car', '.', '<eos>'], ['<sos>', 'the', 'black', 'dog', 'runs', 'through', 'the', 'water', '.', '<eos>'], ['<sos>', 'a', 'man', 'is', 'drilling', 'through', 'the', 'frozen', 'ice', 'of', 'a', 'pond', '.', '<eos>'], ['<sos>', 'two', 'large', 'tan', 'dogs', 'play', 'along', 'a', 'sandy', 'beach', '.', '<eos>'], ['<sos>', 'a', 'person', 'in', 'blue', 'and', 'red', 'ice', 'climbing', 'with', 'two', 'picks', '.', '<eos>'], ['<sos>', 'three', 'people', 'walking', 'on', 'a', 'path', 'in', 'a', 'meadow', '.', '<eos>'], ['<sos>', 'a', 'man', 'in', 'black', 'attire', 'shovels', 'snow', 'into', 'the', 'street', ',', 'disregarding', 'all', 'public', 'safety', '.', '<eos>'], ['<sos>', 'a', 'couple', 'stands', 'behind', 'their', 'wedding', 'cake', '.', '<eos>'], ['<sos>', 'a', 'wet', 'black', 'dog', 'is', 'carrying', 'a', 'green', 'toy', 'through', 'the', 'grass', '.', '<eos>'], ['<sos>', 'villagers', 'selling', 'their', 'crops', 'at', 'the', 'market', '.', '<eos>'], ['<sos>', 'in', 'a', 'crowded', 'concert', 'a', 'man', 'in', 'white', 'is', 'approaching', 'the', 'main', 'singer', 'who', 'is', 'wearing', 'a', 'yellow', 'shirt', '.', '<eos>'], ['<sos>', 'a', 'boy', 'jumps', 'on', 'his', 'skateboard', 'while', 'a', 'crowd', 'watches', '<eos>'], ['<sos>', 'a', 'man', 'and', 'a', 'baby', 'are', 'in', 'a', 'yellow', 'kayak', 'on', 'water', '.', '<eos>'], ['<sos>', 'two', 'people', 'are', 'sitting', 'on', 'a', 'bench', ',', 'and', 'one', 'women', 'is', 'standing', 'by', 'them', '.', '<eos>'], ['<sos>', 'a', 'construction', 'site', 'on', 'a', 'street', 'with', 'three', 'men', 'working', '.', '<eos>'], ['<sos>', 'two', 'men', 'sitting', 'on', 'a', 'bench', 'talking', ',', 'with', 'a', 'billboard', 'advertisement', 'for', 'glasses', 'in', 'the', 'background', '.', '<eos>'], ['<sos>', 'a', 'group', 'of', 'youths', 'march', 'down', 'the', 'street', 'waving', 'flags', 'showing', 'the', 'color', 'spectrum', '.', '<eos>'], ['<sos>', 'three', 'old', 'men', 'are', 'watching', 'another', 'man', 'prepare', 'fish', '.', '<eos>'], ['<sos>', 'a', 'woman', 'in', 'a', 'white', 'tank', 'top', 'with', 'a', 'green', 'flowing', 'skirt', ',', 'on', 'stage', 'singing', 'a', 'song', '.', '<eos>'], ['<sos>', 'two', 'men', 'and', 'two', 'women', 'sitting', 'on', 'steps', 'outdoors', '.', '<eos>'], ['<sos>', 'a', 'brown', 'and', 'black', 'lab', 'are', 'outside', 'and', 'the', 'black', 'lab', 'is', 'catching', 'a', 'toy', 'in', 'its', 'mouth', '.', '<eos>'], ['<sos>', 'hockey', 'goalie', 'boy', 'in', 'red', 'jacket', 'crouches', 'by', 'goal', ',', 'with', 'stick', '.', '<eos>'], ['<sos>', 'the', 'man', 'with', 'the', 'backpack', 'is', 'sitting', 'in', 'a', 'buildings', 'courtyard', 'in', 'front', 'of', 'an', 'art', 'sculpture', 'reading', '.', '<eos>']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_data = dict()\n",
        "total_data['en_tokens'] = train_data_tokens['en_tokens'] + test_data_tokens['en_tokens'] + valid_data_tokens['en_tokens']\n",
        "print(len( train_data_tokens['en_tokens']+test_data_tokens['en_tokens']))\n",
        "print(len(total_data['en_tokens']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVm6onukzWaq",
        "outputId": "7d1f8a5d-ad81-4564-dd53-aa4dcb52cb7e"
      },
      "execution_count": 302,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30014\n",
            "31014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create vocabulary set"
      ],
      "metadata": {
        "id": "dD5_fZoNMh09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_freq = 2\n",
        "unk_token = \"<unk>\" # unknown token, which would be used if we find any word that is out of vocabulary set.\n",
        "pad_token = \"<pad>\" # pad token, to make the length of each setence the same\n",
        "\n",
        "special_tokens = [\n",
        "    unk_token,\n",
        "    pad_token,\n",
        "    sos_token,\n",
        "    eos_token,\n",
        "]\n",
        "\n",
        "# https://pytorch.org/text/stable/vocab.html\n",
        "  # torchtext.vocab.build_vocab_from_iterator\n",
        "  # Build a Vocab from an iterator\n",
        "  # iterator – Iterator used to build Vocab. Must yield list or iterator of tokens.\n",
        "  #min_freq – The minimum frequency needed to include a token in the vocabulary.\n",
        "  #specials – Special symbols to add. The order of supplied tokens will be preserved.\n",
        "  #special_first – Indicates whether to insert symbols at the beginning or at the end.\n",
        "  #max_tokens – If provided, creates the vocab from the max_tokens - len(specials) most frequent\n",
        "\n",
        "  ## return : A Vocab object\n",
        "\n",
        "en_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
        "    train_data[\"en_tokens\"], # list\n",
        "    min_freq=min_freq,\n",
        "    specials=special_tokens,\n",
        ")\n",
        "\n",
        "de_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
        "    train_data[\"de_tokens\"], # list\n",
        "    min_freq=min_freq,\n",
        "    specials=special_tokens,\n",
        ")\n",
        "\n",
        "assert en_vocab[unk_token] == de_vocab[unk_token]\n",
        "assert en_vocab[pad_token] == de_vocab[pad_token]\n",
        "\n",
        "unk_index = en_vocab[unk_token]\n",
        "pad_index = en_vocab[pad_token]\n",
        "\n",
        "### torchtext.vocab.set_default_index\n",
        "## Value of default index. This index will be returned when OOV token is queried.\n",
        "## OOV: Out of Vocabulary tokens\n",
        "en_vocab.set_default_index(unk_index)\n",
        "de_vocab.set_default_index(unk_index)"
      ],
      "metadata": {
        "id": "J9Dk3IaVMUi8"
      },
      "execution_count": 303,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Let's investigate en_vocab and de_vocab together\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xmnpK8rlOvY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for all the attributes and functions of the vocab class, look at the below link\n",
        "#https://pytorch.org/text/stable/vocab.html\n",
        "# for example, let's find the indices of 'like' and 'man' from 'en-vocab' class\n",
        "en_vocab.lookup_indices(['like', 'man'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JftN9IyOEKW",
        "outputId": "96667800-eb81-4e61-b97c-170b725dbea2"
      },
      "execution_count": 304,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[340, 9]"
            ]
          },
          "metadata": {},
          "execution_count": 304
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# map tokens of sentences to the corresponding indices in vocabulary dictionary\n",
        "def map_tokens_to_indices(vocab, en_vocab_dict, de_vocab_dict):\n",
        "\n",
        "  vocab_indices = dict(en_ids = [], de_ids = [])\n",
        "\n",
        "  for en_vocab, de_vocab in tqdm(zip(vocab['en_tokens'], vocab['de_tokens']), desc = 'number of setences', ascii= True, total = len(vocab['en_tokens'])):\n",
        "    en_ids = en_vocab_dict.lookup_indices(en_vocab)\n",
        "    de_ids = de_vocab_dict.lookup_indices(de_vocab)\n",
        "\n",
        "    vocab_indices['en_ids'].append(en_ids)\n",
        "    vocab_indices['de_ids'].append(de_ids)\n",
        "\n",
        "  return vocab_indices"
      ],
      "metadata": {
        "id": "6IYByIhrQ91a"
      },
      "execution_count": 305,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 1:\n",
        "  train_data_setences, test_data_setences, valid_data_setences = dataset[\"train\"], dataset[\"validation\"], dataset['test']\n",
        "  #### update train, test, and valid data with tokenizing sentences ###\n",
        "  nlp_process_tools = {'en_nlp':en_nlp, 'de_nlp':de_nlp} # NLP processing spacy tools\n",
        "  train_data_tokens = tokenize_sentences(train_data_setences, **nlp_process_tools)\n",
        "  test_data_tokens = tokenize_sentences(test_data_setences, **nlp_process_tools)\n",
        "  valid_data_tokens = tokenize_sentences(valid_data_setences, **nlp_process_tools)\n",
        "\n",
        "vocabulary_dictionaries = {'en_vocab_dict':en_vocab, 'de_vocab_dict':de_vocab}\n",
        "## update train, test, valid data by mapping tokens to indices\n",
        "train_data_ids = map_tokens_to_indices(train_data_tokens, **vocabulary_dictionaries)\n",
        "test_data_ids = map_tokens_to_indices(test_data_tokens, **vocabulary_dictionaries)\n",
        "valid_data_ids = map_tokens_to_indices(valid_data_tokens, **vocabulary_dictionaries)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqExhRx-SRjB",
        "outputId": "049fbbf2-b0f8-4908-f36d-023051e416a0"
      },
      "execution_count": 306,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "number of sentences: 100%|##########| 29000/29000 [00:02<00:00, 14213.80it/s]\n",
            "number of sentences: 100%|##########| 1014/1014 [00:00<00:00, 15156.62it/s]\n",
            "number of sentences: 100%|##########| 1000/1000 [00:00<00:00, 14929.86it/s]\n",
            "number of setences: 100%|##########| 29000/29000 [00:02<00:00, 13816.70it/s]\n",
            "number of setences: 100%|##########| 1014/1014 [00:00<00:00, 62270.67it/s]\n",
            "number of setences: 100%|##########| 1000/1000 [00:00<00:00, 67318.90it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## print examples of token-to-indice mapped setences\n",
        "print('################ original english and german sentences ###################')\n",
        "print(train_data_setences['en'][0])\n",
        "print(train_data_setences['de'][0])\n",
        "print('############## after tokenizing the sentences ############')\n",
        "print(train_data_tokens['en_tokens'][0])\n",
        "print(train_data_tokens['de_tokens'][0])\n",
        "print(' ################# after mapping tokens to indcies ########### ')\n",
        "print(train_data_ids['en_ids'][0]) # indices by English dictionary\n",
        "print(train_data_ids['de_ids'][0]) # indices by German dictionary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX_a9irFUOLa",
        "outputId": "0e024492-e41a-42c1-a66d-6936e0d3544b"
      },
      "execution_count": 307,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################ original english and german sentences ###################\n",
            "Two young, White males are outside near many bushes.\n",
            "Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\n",
            "############## after tokenizing the sentences ############\n",
            "['<sos>', 'two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.', '<eos>']\n",
            "['<sos>', 'zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.', '<eos>']\n",
            " ################# after mapping tokens to indcies ########### \n",
            "[2, 16, 24, 15, 25, 778, 17, 57, 80, 202, 1312, 5, 3]\n",
            "[2, 18, 26, 253, 30, 84, 20, 88, 7, 15, 110, 7647, 3171, 4, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_type = \"torch\"\n",
        "format_columns = [\"en_ids\", \"de_ids\"]\n",
        "train_data_ids = h_dataset.from_dict(train_data_ids)\n",
        "test_data_ids = h_dataset.from_dict(test_data_ids)\n",
        "valid_data_ids = h_dataset.from_dict(valid_data_ids)\n",
        "\n",
        "train_data_ids = train_data_ids.with_format(\n",
        "    type=data_type, columns=format_columns, output_all_columns=True\n",
        ")\n",
        "\n",
        "test_data_ids = test_data_ids.with_format(\n",
        "    type=data_type, columns=format_columns, output_all_columns=True\n",
        ")\n",
        "\n",
        "valid_data_ids = valid_data_ids.with_format(\n",
        "    type=data_type, columns=format_columns, output_all_columns=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "kFAGYMCi5TRs"
      },
      "execution_count": 308,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding to all the setences so that the length of each sentence is the same\n"
      ],
      "metadata": {
        "id": "2dJ80V6kWIE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########### get collate_function ####\n",
        "def get_collate_fn(pad_index):\n",
        "    def collate_fn(batch):\n",
        "        batch_en_ids = [example[\"en_ids\"] for example in batch]\n",
        "        batch_de_ids = [example[\"de_ids\"] for example in batch]\n",
        "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n",
        "        batch_de_ids = nn.utils.rnn.pad_sequence(batch_de_ids, padding_value=pad_index)\n",
        "        batch = {\n",
        "            \"en_ids\": batch_en_ids,\n",
        "            \"de_ids\": batch_de_ids,\n",
        "        }\n",
        "\n",
        "        return batch\n",
        "    print(pad_index)\n",
        "    return collate_fn\n",
        "\n",
        "### create dataloader ####\n",
        "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
        "    collate_fn = get_collate_fn(pad_index)\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        collate_fn=collate_fn,\n",
        "        shuffle=shuffle,\n",
        "    )\n",
        "    return data_loader"
      ],
      "metadata": {
        "id": "uffVPoVabO7C"
      },
      "execution_count": 309,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create DataLoader to train model"
      ],
      "metadata": {
        "id": "m_c9ke4UfMbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "# create dataloader for train, test, valid data\n",
        "\n",
        "train_data_loader = get_data_loader(train_data_ids, batch_size, pad_index, shuffle=True)\n",
        "valid_data_loader = get_data_loader(valid_data_ids, batch_size, pad_index)\n",
        "test_data_loader = get_data_loader(test_data_ids, batch_size, pad_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfRyNGB3aCsW",
        "outputId": "fb8a16e4-37b5-4c28-dd75-3067221869c3"
      },
      "execution_count": 310,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "1\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now, ready to train NLP models"
      ],
      "metadata": {
        "id": "eTQa50pWfQp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## check data with dataloader\n",
        "print(len(tarin_dataloader))\n",
        "for i, train_data in enumerate(train_data_loader):\n",
        "  print(train_data['en_ids'].shape)\n"
      ],
      "metadata": {
        "id": "LKFswYk9anSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#### extra function ###########\n",
        "def add_pads_to_setences(data, pad_value = 1):\n",
        "\n",
        "  L = len(data['en_ids'])\n",
        "\n",
        "  assert L == len(data['de_ids'])\n",
        "\n",
        "  _en_ids = [torch.tensor(data['en_ids'][i]) for i in range(L)]\n",
        "  _de_ids = [torch.tensor(data['de_ids'][i]) for i in range(L)]\n",
        "\n",
        "  en_ids_with_pads = nn.utils.rnn.pad_sequence(_en_ids, padding_value = pad_value)\n",
        "  de_ids_with_pads = nn.utils.rnn.pad_sequence(_de_ids, padding_value = pad_value)\n",
        "\n",
        "  return {'en_ids':en_ids_with_pads.T, 'de_ids':de_ids_with_pads.T}\n",
        "\n",
        "pad_index = en_vocab[pad_token]\n",
        "## add pads to all the sentences so that the length of each setence is the same\n",
        "train_data = add_pads_to_setences(train_data_ids, pad_value = pad_index)\n",
        "test_data = add_pads_to_setences(test_data_ids, pad_value = pad_index)\n",
        "valid_data = add_pads_to_setences(valid_data_ids, pad_value = pad_index)"
      ],
      "metadata": {
        "id": "7qlI4d6T4QCp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}